{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0611e",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e5fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc58a9",
   "metadata": {},
   "source": [
    "Inherit from `torch.nn.Module` to build a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b35b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # initialize parent class\n",
    "        super().__init__()\n",
    "        # Define layers\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_inputs, 8),  # First layer with 32 neurons\n",
    "            torch.nn.ReLU(),                   # Activation function for first layer\n",
    "\n",
    "            # Second hidden layer\n",
    "            torch.nn.Linear(8, 4),           # Second layer with 16 neurons\n",
    "            torch.nn.ReLU(),                   # Activation function for second layer\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(4, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The output of the last layer is called the logits\n",
    "        logits = self.layers(x) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e13c3",
   "metadata": {},
   "source": [
    "Now we can initialize the model and take a look at its layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff92fd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNeuralNetwork(num_inputs=5, num_outputs=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ff155",
   "metadata": {},
   "source": [
    "You may have heard of models like Google's Gemini or OpenAI's GPT4 being having been trained on billions of *parameters*. In this context, parameters are simply the tweak-able values to reduce your loss values. The model defined above isn't a crazy-complex model, so it's relatively easier to calculate the number of weights in it. \n",
    "\n",
    "Before using code to see the number of parameters, let's calculate the value manually by looking at each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf36662",
   "metadata": {},
   "source": [
    "The first layer has 5 `in_features` and 8 `out_features`, which means that the input is a `1 x 5` vector, and after it goes through the first linear transformation, it should becomes a `1 x 8` vector. Therefore, the shape of the weights should be `5 x 8`, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1693a127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer = model.layers[0]\n",
    "first_layer.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4144a",
   "metadata": {},
   "source": [
    "Well that's not the case. As you can see, the shape of the first layer is equal to the *transpose* of what we expected. \n",
    "\n",
    "This is because PyTorch's `nn.Linear` stores weights from the layer's POV. \n",
    "Let's revisit how we initialized the first layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5255626b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=8, bias=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(5, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20393253",
   "metadata": {},
   "source": [
    "If you look at the [official docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html) for `torch.nn.Linear`, under `Variables` > `weights`, it writes: \n",
    "\n",
    "> weight (torch.Tensor) â€“ the learnable weights of the module of shape (out features, in features).\n",
    "\n",
    "So mathematically, what's really happening under the hood is:\n",
    "\n",
    "$z = W^TX + b$\n",
    "\n",
    "Where the weight matrix is transposed. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c85ce",
   "metadata": {},
   "source": [
    "What doesn't change is the fact that the weights of the first layer will contribute `5 * 8 = 40` or `8 * 5 = 40` parameters, along with an extra `8` paramters from the bias, 1 for each output feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: shape=(8, 5), num_params=40\n",
      "bias: shape=(8,), num_params=8\n"
     ]
    }
   ],
   "source": [
    "for name, param in first_layer.named_parameters():\n",
    "    print(f\"{name}: shape={tuple(param.shape)}, num_params={param.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f528b3",
   "metadata": {},
   "source": [
    "In total, the first layer will have `40 + 8 = 48` parameters.\n",
    "\n",
    "The same kind of calculation goes for the next two layers defined with `torch.nn.Linear`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2d400",
   "metadata": {},
   "source": [
    "Since the second layer takes in the linear transformation from the first linear layer `first_layer`, the shape of the input will be `1 x 8`. Since it needs to transform that input into a `1 x 4` vector, the shape of the matrix will have to be `8 x 4`. However, because of how PyTorch's `nn.Linear` module stores its weights, the shape of the weights will be the transposition of the matrix used to calculate the layer's output -- `4 x 8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0b67e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_linear_layer = model.layers[2]\n",
    "second_linear_layer.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cfca8",
   "metadata": {},
   "source": [
    "Since this layer also has a bias, one for each output neuron, the total number of parameters from this layer will be `4 * 8 + 4 = 36`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a94355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: shape=(4, 8), num_params=32\n",
      "bias: shape=(4,), num_params=4\n"
     ]
    }
   ],
   "source": [
    "for name, param in second_linear_layer.named_parameters():\n",
    "    print(f\"{name}: shape={tuple(param.shape)}, num_params={param.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba4a4d",
   "metadata": {},
   "source": [
    "Since this calculation remains the same, I'll just show the code for calcaulating the number of paramters in the last layer defined using `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9368b479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_linear_layer = model.layers[4]\n",
    "third_linear_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f404ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: shape=(2, 4), num_params=8\n",
      "bias: shape=(2,), num_params=2\n"
     ]
    }
   ],
   "source": [
    "for name, param in third_linear_layer.named_parameters():\n",
    "    print(f\"{name}: shape={tuple(param.shape)}, num_params={param.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15526ad1",
   "metadata": {},
   "source": [
    "In total, we can expect to see `48 + 36 + 10 = 94` parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b7ba476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters in the model: 94\n"
     ]
    }
   ],
   "source": [
    "model_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable parameters in the model: {model_total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c288e2d",
   "metadata": {},
   "source": [
    "One thing to note here is that each `nn.Linear` layer has `requires_grad` set to `True`, since calculating the gradients for all 94 paramters will take a long time, even for a model this small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3721df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 4.4285e-01, -3.0492e-01, -3.2734e-01,  3.2656e-01, -1.0983e-01],\n",
      "        [-1.3585e-01, -8.8028e-02,  1.6133e-01,  2.7886e-02, -1.9292e-01],\n",
      "        [ 1.3574e-01,  2.4653e-01,  7.3773e-02, -1.2550e-01,  2.4974e-01],\n",
      "        [ 4.3568e-01,  3.3548e-01,  5.4901e-04,  3.9819e-01,  3.4916e-01],\n",
      "        [-6.4779e-02, -3.4380e-01,  2.1437e-01, -1.7376e-01,  3.3959e-01],\n",
      "        [ 4.1931e-01, -6.9882e-02,  2.9337e-01, -3.6723e-03,  7.8795e-05],\n",
      "        [ 1.8392e-01,  1.5690e-01,  1.9126e-01,  2.6826e-01, -2.2186e-02],\n",
      "        [-3.1236e-01,  4.4170e-01, -1.9657e-01,  9.7641e-02, -2.2728e-02]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011b161",
   "metadata": {},
   "source": [
    "The weights are initialized with random small numbers every time you initialize a new model. To fix the randomly-initilized values, you can seed the random number generator with `manual_seed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05c83b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3419,  0.3712, -0.1048,  0.4108, -0.0980],\n",
      "        [ 0.0902, -0.2177,  0.2626,  0.3942, -0.3281],\n",
      "        [ 0.3887,  0.0837,  0.3304,  0.0606,  0.2156],\n",
      "        [-0.0631,  0.3448,  0.0661, -0.2088,  0.1140],\n",
      "        [-0.2060, -0.0524, -0.1816,  0.2967, -0.3530],\n",
      "        [-0.2062, -0.1263, -0.2689,  0.0422, -0.4417],\n",
      "        [ 0.4039, -0.3799,  0.3453,  0.0744, -0.1452],\n",
      "        [ 0.2764,  0.0697,  0.3613,  0.0489, -0.1410]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "my_model = MyNeuralNetwork(num_inputs=5, num_outputs=2)\n",
    "print(my_model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb133c22",
   "metadata": {},
   "source": [
    "## Forward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca04486",
   "metadata": {},
   "source": [
    "With a model initialized, you can generate output vectors by feeding the model with input vectors. This \"feeding\" of the input vector that results in an output is called the *forward pass*. Since `my_model` takes in 5 features as its input, the input will be a randomly generated `1 x 5` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aa25816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229]]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "X = torch.randn((1, 5))\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52e61bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3806, 0.3704]], grad_fn=<AddmmBackward0>), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_output = my_model(X)\n",
    "forward_output, forward_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a0c3e",
   "metadata": {},
   "source": [
    "When you pass the input to the model like `my_model(X)`, this will automatically call the `forward()` method that was defined when we first defined the model class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fecc02",
   "metadata": {},
   "source": [
    "The output tensor contains a `grad_fn` property, which shows the operation that took to result in the output. In the case of the above code cell, that operation is `Addmm`, which is matrix multiplication `mm` and addition `Add`. This information will be used when we want to optimize the model's parameters using backpropagation, where PyTorch will handle the gradient calculation for you. \n",
    "\n",
    "However, in the case that you don't intend to perform backprop (e.g., you have a finalized model and you don't intend to optimize it again), you can use the directive `with torch_no_grad()`, which will save you a lot of compute power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e90a84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3806, 0.3704]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    forward_output_no_grad = my_model(X)\n",
    "    print(forward_output_no_grad, forward_output_no_grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a194c0d",
   "metadata": {},
   "source": [
    "## Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814ddec",
   "metadata": {},
   "source": [
    "We have the outputs, now what? \n",
    "Well, if you look at the output tensor, it has two numbers. You can think of each number as a value that can be translated to a probability -- the probability of the input belonging in one of those categories. To see this, the output tensor would need to go through a `softmax` operation, which is essentially a function in math that takes in a vector of numbers into a probability distribution. Thus, the sum of the numbers of `softmax`'s output will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cf5c14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5025, 0.4975]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    forward_output_no_grad = my_model(X)\n",
    "    softmax_output = torch.nn.functional.softmax(forward_output_no_grad, dim=1)\n",
    "    print(softmax_output, softmax_output.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819dd5b",
   "metadata": {},
   "source": [
    "One thing interesting to note here is that the probability of the input belonging in a particular category is 50:50, which reflects the model weights' random initialization.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
