{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42d113c",
   "metadata": {},
   "source": [
    "# PyTorch's Autograd Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d7a1a",
   "metadata": {},
   "source": [
    "## Logistic Regression Forward Pass (Initial Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94f282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b017ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true label\n",
    "y = torch.tensor([1.0])\n",
    "# input feature\n",
    "x1 = torch.tensor([1.3])\n",
    "# weight\n",
    "w1 = torch.tensor([2.0])\n",
    "# bias\n",
    "b1 = torch.tensor([0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11659e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted sum\n",
    "z1 = w1 * x1 + b1\n",
    "# sigmoid activation\n",
    "y_hat = torch.sigmoid(z1)\n",
    "# binary cross-entropy loss\n",
    "loss = F.binary_cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b1109b",
   "metadata": {},
   "source": [
    "The non-linear sigmoid activation function is applied after the linear transformation `w1 * x1 + b1` \n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16153fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted sum (z1): tensor([3.1000])\n",
      "Predicted value (y_hat): tensor([0.9569])\n",
      "Loss: 0.0440639853477478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weighted sum (z1): {z1}\")\n",
    "print(f\"Predicted value (y_hat): {y_hat}\")\n",
    "print(f\"Loss: {loss.item()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7528cac",
   "metadata": {},
   "source": [
    "The goal is to **minimize** the loss, but how do we do that? \n",
    "\n",
    "Neural networks use *gradient descent*, which uses the gradient to determine how much, and in which direction, the network's weights should be tweaked. A gradient is essentially a vector with numbers that represent, \"How much, in which direction, should the weights be tweaked to minimize the loss?\"\n",
    "\n",
    "Now the basic mathematical concept behind calculating gradients is in the *chain rule* of calculus, where you get the derivative of the loss with respsect to the weights. With the plain vanilla approach implemented above, we'd have to go backwards through each line of code and calculate the derivative of the result with respect to[^1] the variable of interest. What's nice about PyTorch is, is that it is an *autograd* engine, which basically means it tracks the calculation done on all your tensors and does the dirty work for you. \n",
    "\n",
    "So if we implement the above calculation again, but with `requires_grad=True` for `w` and `b`, we'll be able to get the \"direction\" and magnitude of change we need to make to the weight(s) and bias(es) to minimize the loss. \n",
    "\n",
    "> The derivative of $y$ with respect to $x$ means: How much does $y$ change if I change $x$ by a certain amount? \n",
    "\n",
    "> Quotes around direction because you actually need to go the opposite direction of the gradient, since the goal is to *minimize* the loss, not make it bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce426cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# true label\n",
    "y = torch.tensor([1.0])\n",
    "# input feature\n",
    "x1 = torch.tensor([1.3])\n",
    "# weight\n",
    "w1 = torch.tensor([2.0], requires_grad=True)\n",
    "# bias\n",
    "b1 = torch.tensor([0.5], requires_grad=True)\n",
    "# weighted sum\n",
    "z1 = w1 * x1 + b1\n",
    "# sigmoid activation\n",
    "y_hat = torch.sigmoid(z1)\n",
    "# binary cross-entropy loss\n",
    "loss = F.binary_cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02ad6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: retain_graph=True is needed here because we'll have to keep the \"tracking info\"\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b1 = grad(loss, b1, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd8bec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial gradient of loss with respect to w1: tensor([-0.0560])\n",
      "Partial gradient of loss with respect to b1: tensor([-0.0431])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Partial gradient of loss with respect to w1: {grad_L_w1[0]}\")\n",
    "print(f\"Partial gradient of loss with respect to b1: {grad_L_b1[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9faa3e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss with respect to w1 using backward(): tensor([-0.0560])\n",
      "Gradient of loss with respect to b1 using backward(): tensor([-0.0431])\n"
     ]
    }
   ],
   "source": [
    "# You can also use the .backward() method on the loss tensor to compute the gradients\n",
    "loss.backward()\n",
    "print(f\"Gradient of loss with respect to w1 using backward(): {w1.grad}\")\n",
    "print(f\"Gradient of loss with respect to b1 using backward(): {b1.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
